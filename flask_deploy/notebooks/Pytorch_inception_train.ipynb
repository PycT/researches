{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil;\n",
    "import math, random;\n",
    "\n",
    "from torch import nn;\n",
    "import torch.optim;\n",
    "from torch.utils.data import DataLoader, Dataset;\n",
    "from torchvision import models, transforms;\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS, default_loader;\n",
    "from torchvision.datasets import ImageFolder;\n",
    "\n",
    "import numpy as np;\n",
    "from tqdm import tqdm_notebook as tqdm;\n",
    "\n",
    "datasets_dir = \"../../../Datasets/STARE\";\n",
    "\n",
    "initial_dir = datasets_dir + \"/all-images\";\n",
    "\n",
    "training_set_dir = datasets_dir + \"/training\";\n",
    "validation_set_dir = datasets_dir + \"/validation\";\n",
    "classes = \\\n",
    "[\n",
    "    \"normal\",\n",
    "    \"abnormal\"\n",
    "];\n",
    "\n",
    "labels_file_path = datasets_dir + \"/all-mg-codes.txt\"\n",
    "\n",
    "prediction_classes_quantity = 2; # \"normal\", \"abnormal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModel(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False):\n",
    "        super(InceptionModel, self).__init__()\n",
    "        model = models.inception_v3(pretrained=pretrained)\n",
    "        model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "        model.fc = nn.Linear(2048, num_classes)\n",
    "        #model.fc = nn.Sigmoid()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare datasets functions\n",
    "\n",
    "def dir_structure(datasets_dir = datasets_dir, training_set_dir = training_set_dir,\\\n",
    "                  validation_set_dir = validation_set_dir):\n",
    "    \n",
    "    if (not os.path.exists(validation_set_dir)):\n",
    "        os.mkdir(validation_set_dir);\n",
    "    \n",
    "    if (not os.path.exists(training_set_dir)):\n",
    "        os.mkdir(training_set_dir);\n",
    "        \n",
    "    for a_class in classes:\n",
    "        \n",
    "        if (not os.path.exists(training_set_dir + \"/\" + a_class)):\n",
    "            os.mkdir(training_set_dir + \"/\" + a_class)\n",
    "        \n",
    "        if (not os.path.exists(validation_set_dir + \"/\" + a_class)):\n",
    "            os.mkdir(validation_set_dir + \"/\" + a_class)\n",
    "        \n",
    "    return True;\n",
    "\n",
    "def make_data_sets_lists(sets_chopper = 0.8, datasets_dir = datasets_dir, labels_file_path = labels_file_path):\n",
    "    \"\"\"\n",
    "    This method is specific to STARE dataset.\n",
    "    Sets_chopper sets the proportion between training and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_file = open(labels_file_path, \"r\");\n",
    "    \n",
    "    normal_set = [];\n",
    "    abnormal_set = [];\n",
    "    \n",
    "    for line in labels_file.readlines():\n",
    "        raw = line.split();\n",
    "        \n",
    "        if (int(raw[1]) == 0):\n",
    "            normal_set.append({\"file\": raw[0], \"label\": 0});\n",
    "        else:\n",
    "            abnormal_set.append({\"file\": raw[0], \"label\": 1});\n",
    "                \n",
    "    random.shuffle(normal_set);\n",
    "    random.shuffle(abnormal_set);\n",
    "    \n",
    "    normal_set_size = len(normal_set);\n",
    "    abnormal_set_size = len(abnormal_set);\n",
    "    \n",
    "    training_normal_set_size = math.floor(normal_set_size * sets_chopper);\n",
    "    validation_normal_set_size = normal_set_size - training_normal_set_size;\n",
    "    \n",
    "    training_abnormal_set_size = math.floor(abnormal_set_size * sets_chopper);\n",
    "    validation_abnormal_set_size = abnormal_set_size - training_abnormal_set_size;\n",
    "    \n",
    "    training_set = normal_set[:training_normal_set_size] + abnormal_set[:training_abnormal_set_size];\n",
    "    validation_set = normal_set[training_normal_set_size:] + abnormal_set[training_abnormal_set_size:];\n",
    "        \n",
    "    return {\"training_set\": training_set, \"validation_set\": validation_set};\n",
    "\n",
    "def fill_sets_dir(whole_set):\n",
    "    \"\"\"\n",
    "    copy files from common folder to training/validation classes folder\n",
    "    \"\"\"\n",
    "    \n",
    "    for sample in whole_set[\"training_set\"]:\n",
    "        shutil.copy(initial_dir + \"/\" + sample[\"file\"] + \".ppm\", training_set_dir + \"/\" + classes[sample[\"label\"]]);\n",
    "\n",
    "    for sample in whole_set[\"validation_set\"]:\n",
    "        shutil.copy(initial_dir + \"/\" + sample[\"file\"] + \".ppm\", validation_set_dir + \"/\" + classes[sample[\"label\"]]);\n",
    "    \n",
    "    return True;\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_structure();\n",
    "#whole_set = make_data_sets_lists();\n",
    "#fill_sets_dir(whole_set);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionModel(prediction_classes_quantity);\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training functions\n",
    "\n",
    "def train_the_model(model, batch_size, patience, epochs_sentence):\n",
    "    \"\"\"\n",
    "    'patience' is a number of interations without loss improvement, after which training is stopped.\n",
    "    \"\"\"    \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True);\n",
    "    \n",
    "    data_transform = transforms.Compose\\\n",
    "    (\n",
    "        [\n",
    "            transforms.Resize((299, 299)),\n",
    "            #transforms.RandomHorizontalFlip();\n",
    "            transforms.RandomRotation(180),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize\\\n",
    "            (\n",
    "                mean = [0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ]\n",
    "    );\n",
    "    \n",
    "    training_dataset = ImageFolder(training_set_dir, transform = data_transform);\n",
    "    training_data_loader = DataLoader(training_dataset, batch_size = batch_size, shuffle = True);\n",
    "\n",
    "    validation_dataset = ImageFolder(validation_set_dir, transform = data_transform);\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle = True);\n",
    "\n",
    "    avg_training_loss_tracker = [];\n",
    "    avg_validation_loss_tracker = [];\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs_sentence + 1)):\n",
    "\n",
    "        training_loss_tracker = [];\n",
    "        validation_loss_tracker = [];\n",
    "        \n",
    "        \"\"\"training block starts\"\"\"\n",
    "        model.train(); #initialize model for training;\n",
    "        \n",
    "        for batch, (samples, labels) in tqdm(enumerate(training_data_loader, 1), \\\n",
    "                                           desc = \"Training\", total = len(training_data_loader)):\n",
    "            \n",
    "            labels = torch.LongTensor(labels);\n",
    "            \n",
    "            outputs, outputs_aux = model(samples);\n",
    "              \n",
    "            loss1 = criterion(outputs, labels);\n",
    "            loss2 = criterion(outputs_aux, labels);\n",
    "            loss = loss1 + 0.4 * loss2;\n",
    "\n",
    "            loss.backward();\n",
    "            optimizer.step();\n",
    "\n",
    "            training_loss_tracker.append(float(loss.item()));\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print(\"Train loss: {}\".format(sum(training_loss_tracker) / len(training_loss_tracker)));\n",
    "            \n",
    "            del samples;\n",
    "            del labels;\n",
    "            \n",
    "        \"\"\"training block ends\"\"\"\n",
    "                                           \n",
    "        \"\"\"validation block starts\"\"\"\n",
    "        model.eval(); #initialize model for validation;\n",
    "        \n",
    "        for batch, (samples, labels) in tqdm(enumerate(validation_data_loader, 1), \\\n",
    "                                           desc = \"Validation\", total = len(validation_data_loader)):\n",
    "            \n",
    "            labels = torch.LongTensor(labels);\n",
    "            \n",
    "            outputs = model(samples);\n",
    "            loss = criterion(outputs, labels);\n",
    "            \n",
    "            _, predictions = torch.max(outputs, 1);\n",
    "            \n",
    "            validation_loss_tracker.append(loss.item());\n",
    "            \n",
    "            running_accuracy = torch.sum(predictions == labels.data);\n",
    "\n",
    "            training_loss_tracker.append(float(loss.item()));\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print()\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print(\"Validity {}\".format(float(running_accuracy) / batch_size));\n",
    "            \n",
    "            del samples;\n",
    "            del labels;\n",
    "            \n",
    "        \"\"\"validation block ends\"\"\"\n",
    "        \n",
    "        avg_training_loss = np.average(training_loss_tracker);\n",
    "        avg_validation_loss = np.average(validation_loss_tracker);\n",
    "        \n",
    "        avg_training_loss_tracker.append(avg_training_loss);\n",
    "        avg_validation_loss_tracker.append(avg_validation_loss);\n",
    "        \n",
    "        early_stopping(avg_validation_loss, model);\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stop.\");\n",
    "            break;\n",
    "            \n",
    "    return model, avg_training_loss_tracker, avg_validation_loss_tracker;\n",
    "        \n",
    "        \n",
    "                                           \n",
    "                                           \n",
    "                                           \n",
    "                            \n",
    "                                           \n",
    "                                           \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410e9249ed6a45e7bca886dffabae040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=77), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247c365c4c8043558d45173e939fe9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.391345133818686\n",
      "Train loss: 1.246032698478666\n",
      "Train loss: 1.2188958932820242\n",
      "Train loss: 1.1686346016722382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce7c2a5a3f1492e905777759e88bff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validity 0.0\n",
      "Validation loss decreased (inf --> 3677.171875).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3182365f7ffe4573a05a7835088d90b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_the_model(model, 8, 8, 77);\n",
    "\n",
    "torch.save(model, \"../models/retina_screening.torch\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
